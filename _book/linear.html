<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>outlier-detection.utf8.md</title>
  <meta name="description" content="This article is concerned with outlier detection using regression in R.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="outlier-detection.utf8.md" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This article is concerned with outlier detection using regression in R." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="outlier-detection.utf8.md" />
  
  <meta name="twitter:description" content="This article is concerned with outlier detection using regression in R." />
  

<meta name="author" content="Samuel Lippl">


<meta name="date" content="2018-09-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="methodology.html">
<link rel="next" href="nonlinear-extensions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Outlier Detection using Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction: neurons, models and outliers</a></li>
<li class="chapter" data-level="2" data-path="methodology.html"><a href="methodology.html"><i class="fa fa-check"></i><b>2</b> Methodology</a><ul>
<li class="chapter" data-level="2.1" data-path="methodology.html"><a href="methodology.html#a-nested-account-of-outlier-detection"><i class="fa fa-check"></i><b>2.1</b> A nested account of outlier detection</a></li>
<li class="chapter" data-level="2.2" data-path="methodology.html"><a href="methodology.html#context-probabilistic-and-cluster-analysis-for-outlier-detection"><i class="fa fa-check"></i><b>2.2</b> Context: Probabilistic and cluster analysis for outlier detection</a></li>
<li class="chapter" data-level="2.3" data-path="methodology.html"><a href="methodology.html#z-scoring-a-heuristic-for-normalization-of-outlier-scores"><i class="fa fa-check"></i><b>2.3</b> Z-Scoring: A heuristic for normalization of outlier scores</a></li>
<li class="chapter" data-level="2.4" data-path="methodology.html"><a href="methodology.html#evaluation-of-outlier-detection-algorithms"><i class="fa fa-check"></i><b>2.4</b> Evaluation of outlier detection algorithms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>3</b> Linear Outlier Detection</a><ul>
<li class="chapter" data-level="3.1" data-path="linear.html"><a href="linear.html#linear-model"><i class="fa fa-check"></i><b>3.1</b> Linear Models</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear.html"><a href="linear.html#motivation"><i class="fa fa-check"></i><b>3.1.1</b> Motivation</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear.html"><a href="linear.html#definition-and-implementation-in-r"><i class="fa fa-check"></i><b>3.1.2</b> Definition and implementation in R</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear.html"><a href="linear.html#example-application"><i class="fa fa-check"></i><b>3.1.3</b> Example application</a></li>
<li class="chapter" data-level="3.1.4" data-path="linear.html"><a href="linear.html#data-compression-and-correction"><i class="fa fa-check"></i><b>3.1.4</b> Data compression and correction</a></li>
<li class="chapter" data-level="3.1.5" data-path="linear.html"><a href="linear.html#discussion"><i class="fa fa-check"></i><b>3.1.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear.html"><a href="linear.html#linear-pca"><i class="fa fa-check"></i><b>3.2</b> Principal Component Analysis</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear.html"><a href="linear.html#motivation-1"><i class="fa fa-check"></i><b>3.2.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear.html"><a href="linear.html#definition"><i class="fa fa-check"></i><b>3.2.2</b> Definition</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear.html"><a href="linear.html#example-application-1"><i class="fa fa-check"></i><b>3.2.3</b> Example application</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear.html"><a href="linear.html#data-compression-and-correction-1"><i class="fa fa-check"></i><b>3.2.4</b> Data compression and correction</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear.html"><a href="linear.html#discussion-1"><i class="fa fa-check"></i><b>3.2.5</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html"><i class="fa fa-check"></i><b>4</b> Nonlinear extensions</a><ul>
<li class="chapter" data-level="4.1" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#kernel-pca"><i class="fa fa-check"></i><b>4.1</b> Kernel PCA</a><ul>
<li class="chapter" data-level="4.1.1" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#motivation-2"><i class="fa fa-check"></i><b>4.1.1</b> Motivation</a></li>
<li class="chapter" data-level="4.1.2" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#definition-1"><i class="fa fa-check"></i><b>4.1.2</b> Definition</a></li>
<li class="chapter" data-level="4.1.3" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#example-application-2"><i class="fa fa-check"></i><b>4.1.3</b> Example application</a></li>
<li class="chapter" data-level="4.1.4" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#discussion-2"><i class="fa fa-check"></i><b>4.1.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#neural-networks"><i class="fa fa-check"></i><b>4.2</b> Neural networks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Linear Outlier Detection</h1>
<p>Regression starts with a linear model – many nonlinear methods are essentially extensions of linear methods. It is therefore sensible to begin this exposition with the linear methods, as well. We will first discuss linear models and then go on to discuss principal component analysis.</p>
<div id="linear-model" class="section level2">
<h2><span class="header-section-number">3.1</span> Linear Models</h2>
<p>Linear Models are the most common regression technique and therefore a fitting start for a discussion of outlier detection algorithms using regression.</p>
<div id="motivation" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Motivation</h3>
<p>Consider a few examples: suppose we compare the performance of students a lecture with their grades after the exam. Alternatively, we compare the level of education with the pay grade or the forecast with the actual weather. In all these cases, our intuition for outliers is rather clear: how strongly does the latter dimension deviate from the prediction of the former? These are <strong>instances</strong> of a directed model which may be implemented by a linear model. We therefore extract a certain feature which should conform to the prediction by the other feature. A larger deviation would imply a stronger possibility that a certain data point is an outlier.</p>
</div>
<div id="definition-and-implementation-in-r" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Definition and implementation in R</h3>

<div class="definition">
<span id="def:unnamed-chunk-5" class="definition"><strong>Definition 3.1  </strong></span>Consider data points <span class="math inline">\(i\in\{1,\dotsc,n\}\)</span> with covariates <span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span> and a predicted variable <span class="math inline">\(Y\in\mathbb{R}^n\)</span>. <em>Outlier detection with a linear model</em> first estimates the least squares fit of the linear model and then predicts all data points from their covariates. The deviation <span class="math inline">\(|\hat{y}_i-y_i|\)</span> is the resulting outlier score.
</div>

<p>The implementation is simple:</p>
<ol style="list-style-type: decimal">
<li><p>Fit a linear model with the function <code>lm</code>.</p></li>
<li><p>Predict the data points and determine the residuals with <code>predict</code>.</p></li>
</ol>
</div>
<div id="example-application" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Example application</h3>
<div class="figure"><span id="fig:lin-ex"></span>
<img src="outlier-detection_files/figure-html/lin-ex-1.png" alt="Outlier detection using linear models" width="672" />
<p class="caption">
Figure 3.1: Outlier detection using linear models
</p>
</div>
<p>The application of this method to the example data can be found in figure <a href="linear.html#fig:lin-ex">3.1</a>. The linear pattern is recognized rather well which is partly due to the evenly distributed outliers. The algorithm fails to recognize the correct patterns in the nonlinear cases for obvious reasons.</p>
</div>
<div id="data-compression-and-correction" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Data compression and correction</h3>
<p>Data compression would be achieved by leaving out the predicted variable and decoding the compressed pattern by predicting it according to the fitted linear model. Data correction would correspond to replacing the true value of the predicted variable by its prediction from the other variables.</p>
</div>
<div id="discussion" class="section level3">
<h3><span class="header-section-number">3.1.5</span> Discussion</h3>
<p>It is important to emphasize at this point that such a simple model will scarcely be useful for many different reasons. From a practical point of view, the dynamics in most real-world datasets cannot be captured by a linear assumption. This restriction applies to all methods in this chapter. However, this method can easily be extended to arbitrary regressions which are able to capture more complex relationships. This enables us to extract features, prevent overfitting and adress other issues with the help of regression methods. On the other hand, special knowledge is necessary to apply a directed model – after all, we need to know which variable to predict. In the aforementioned examples, this has worked – in particular, these methods may often be applied in a spatiotemporal context <span class="citation">(Aggarwal <a href="#ref-Aggarwal2017">2017</a>, ch. 9 and 11)</span> in which we predict data at time point <span class="math inline">\(t\)</span> with the data at time point <span class="math inline">\(t-1\)</span>, for instance. In many cases, however, there is no obvious special variable to predict. <span class="citation">Paulheim and Meusel (<a href="#ref-Paulheim2015">2015</a>)</span> presented the <em>ALSO approach</em> which essentially fits a regression for every variable. This approach has the advantage that it requires no special knowledge while retaining the adaptability of these regression methods.</p>
<p>It is also important that many regression methods are susceptible to the very problem they must adress in this context, namely outliers. <span class="citation">Fahrmeir et al. (<a href="#ref-Fahrmeir2013">2013</a>)</span> point out that even a single outlier may affect the fitted hyperplane to an arbitrarily large degree. In order to solve this issue, it is possible to use an iterated approach where in the first step, we attempt to characterize the outliers and then fit another regression after removing the obvious outliers. Several iterations would possibly adress this issue. This approach may be discussed within the more general context of <em>ensemble methods</em> which <span class="citation">Aggarwal (<a href="#ref-Aggarwal2017">2017</a>)</span> adresses in chapter 6. This is, however beyond the scope of this report.</p>
</div>
</div>
<div id="linear-pca" class="section level2">
<h2><span class="header-section-number">3.2</span> Principal Component Analysis</h2>
<div id="motivation-1" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Motivation</h3>
<p>Another possibility to remove the directedness of the approach in the last section is by considering the <em>orthogonal prediction error</em>. Consider figure <a href="linear.html#fig:ortho">3.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:ortho"></span>
<img src="figs/ortho.png" alt="Orthogonal error (blue) and directed error (red)" width="50%" />
<p class="caption">
Figure 3.2: Orthogonal error (blue) and directed error (red)
</p>
</div>
<p>A directed regression would attempt to minimize the length of one of the red arrows. In contrast, <em>orthogonal regression</em> attempts to minimize the orthogonal error, which corresponds to the blue arrow. This regression method is symmetrical which removes the choice of the predicted variable. It can be shown that this task is equivalent to the following two tasks:</p>
<ul>
<li>Solving a regression problem where all variables are used as covariates, the predicted variable <span class="math inline">\(Y\)</span> is constantly zero and the regression coefficients as a vector have unit norm,</li>
<li>Determining the linear combination with the least variance, i. e. the last principal component.</li>
</ul>
<p>Principal component analysis (PCA) is therefore a natural extension of orthogonal regression which we will explore in this section.</p>
</div>
<div id="definition" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Definition</h3>
<p>As PCA is covered in most undergraduate degrees in statistics, I will leave out the mathematical definition and refer the interested reader to chapter 11 of <span class="citation">Härdle and Simar (<a href="#ref-Hardle2015">2015</a>)</span>. Intuitively, a PCA attempts to capture as much information within the first linear combinations of the variables as possible. The different principal components are therefore uncorrelated. This approach is especially sensible in cases where high dimensionality poses a problem, in particular if humans need to deal with the data. Usually, a cutoff point <span class="math inline">\(k\)</span> is chosen and only the first <span class="math inline">\(k\)</span> prinicipal components are retained. In order to chose this cutoff point, different techniques are possible.</p>

<div class="definition">
<span id="def:unnamed-chunk-6" class="definition"><strong>Definition 3.2  </strong></span>Consider data points <span class="math inline">\(i\in\{1,\dotsc,n\}\)</span> with the observations <span class="math inline">\(X\in\mathbb{R}^{n\times p}\)</span>. <em>Outlier detection with PCA</em> first identifies the matrices
<span class="math display">\[
  P=\begin{pmatrix}P_1&amp;\dotsb&amp;P_p\end{pmatrix}\in\mathbb{R}^{p\times p},\Lambda = \text{diag}(\lambda_1,\dotsc,\lambda_p),\lambda_i\ge\lambda_{i+1}\ge0
\]</span>
such that
<span class="math display">\[
  P^TP=I, X^TX=P\Lambda^2P^T.
\]</span>
The principal components <span class="math inline">\(T_j:=XP_j\)</span> are computed. By assumption, the last principal components are mainly driven by noise. If they are larger, the outlier score should be as well. As <span class="math inline">\(\lambda_i\)</span> is the standard deviation of <span class="math inline">\(P_i\)</span>, the outlier score is defined as
<span class="math display">\[
  \left(\sum_{i=k+1}^p\lambda_i^{-2}T_i^2\right)^{\frac{1}{2}}.
\]</span>
</div>

<p>In the case of outlier analysis, the cutoff point is often assumed to be zero, i. e. we computed the weighted average squares of all principal components. This is because the cutoff point introduces a new parameter and these parameters are especially difficult to fit in the context of outlier analysis. On the other hand, we can scarcely make an informed choice regarding the cutoff point as principal components almost never have an intuitive interpretation. In particular, discarding the need for an informed choice was our initial motivation for principal components analysis. Furthermore, the standard deviation of the first principal components is usually large and their variation does not have a huge influence on the outlier score, anyway.</p>
<p>In R, this method may be implemented by the following steps:</p>
<ol style="list-style-type: decimal">
<li>Fit the principal components analysis using <code>prcomp</code>, e. g.: <code>model &lt;- prcomp(x)</code>.</li>
<li>Predict the principal components using <code>predict</code>, e. g.: <code>pcs &lt;- predict(model)</code>.</li>
<li>Compute the score. You can access the standard deviations of the different principal components via the <code>sdev</code> element of the list which <code>prcomp</code> returns, e. g.: <code>score &lt;- pcs ^ 2 %*% model$sdev</code>.</li>
</ol>
</div>
<div id="example-application-1" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Example application</h3>
<p>Figure <a href="linear.html#fig:pca-ex">3.3</a> shows the result of the PCA outlier detection.</p>
<div class="figure"><span id="fig:pca-ex"></span>
<img src="outlier-detection_files/figure-html/pca-ex-1.png" alt="Outlier detection using principal components analysis" width="672" />
<p class="caption">
Figure 3.3: Outlier detection using principal components analysis
</p>
</div>
<p>Notably, the outlier score now has elliptic levels (in contrast, the linear model yielded straight levels). This demonstrates that both principal components are part of our outlier score. In the case of the linear pattern, the assumption that both dimensions are relevant to the outliers is simply wrong which is why the pattern recognition is not accurate. In the circular case, the method actually recognizes that the points outside the circle are outliers. Due to the method’s linear assumption, it is not able to recognize that there are outliers within the circle as well. The method is not adept at recognizing the quadratic and sinusoidal pattern either.</p>
</div>
<div id="data-compression-and-correction-1" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Data compression and correction</h3>
<p>With principal component analysis, data may be compressed arbitrarily strongly, as we may discard as many principal components as we would like. Encoding would therefore happen by computing <span class="math inline">\(T=XP\)</span> and discarding the last principal components. <span class="math inline">\(\tilde{X}=TP^T\)</span> would decode <span class="math inline">\(T\)</span>. The data may also be corrected in this way. The underlying assumption is that the last components are driven by noise and the correct value is therefore likely to be constant, i. e. zero.</p>
<p>Both compression and correction require a <em>hard PCA</em>, i. e. one with non-trivial cutoff points. This comes with its advantages and disadvantages.</p>
</div>
<div id="discussion-1" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Discussion</h3>
<p>In the context of outlier analysis, a soft PCA is mostly a better choice than a hard PCA for the aforementioned reasons and we have solved the problem of assymmetry which we had with the linear model. On the other hand, the PCA is still restricted by linear assumptions. In particular, the soft PCA is equivalent to the <em>Mahalanobis score</em>, a simple probabilistic outlier score. The point of using principal components analysis, however, is that we can find a non-linear extension and the so called <em>kernel trick</em> which enables us to do that will be the subject of the first section in the next chapter. This computational technique was developed for Support Vector Machines (SVMs), a popular Machine Learning Algorithm <span class="citation">(Aggarwal <a href="#ref-Aggarwal2015">2015</a>, sec. 10.6)</span>. Indeed, SVMs have found their way into outlier analysis, as well. However, their performance depends on the choice of certain hyperparameters in an intransparent manner and this method is therefore less reliable. <span class="citation">(Manevitz and Yousef <a href="#ref-Manevitz2001">2001</a>)</span> As introducing SVMs would require introducing Lagrange optimizers, I have decided to discard this technique for the present report.</p>
<p>Finally, I will point out that besides the non-linear extensions which we will discuss, techniques have been developed to fit PCA to noisy or missing data <span class="citation">(Bailey <a href="#ref-Bailey2012">2012</a>)</span> which can be of use in the context of outlier detection, as well.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Aggarwal2017">
<p>Aggarwal, Charu C. 2017. <em>Outlier Analysis</em>. Springer Publishing Company, Incorporated.</p>
</div>
<div id="ref-Paulheim2015">
<p>Paulheim, Heiko, and Robert Meusel. 2015. “A decomposition of the outlier detection problem into a set of supervised learning problems.” <em>Machine Learning</em> 100 (2):509–31. <a href="https://doi.org/10.1007/s10994-015-5507-y" class="uri">https://doi.org/10.1007/s10994-015-5507-y</a>.</p>
</div>
<div id="ref-Fahrmeir2013">
<p>Fahrmeir, Ludwig, Thomas Kneib, Stefan Lang, and Brian Marx. 2013. <em>Regression - Models, Methods and Applications</em>.</p>
</div>
<div id="ref-Hardle2015">
<p>Härdle, Wolfgang Karl, and Léopold Simar. 2015. <em>Applied Multivariate Statistical Analysis</em>. 4th ed. Berlin, Heidelberg: Springer.</p>
</div>
<div id="ref-Aggarwal2015">
<p>Aggarwal, Charu C. 2015. <em>Data Mining</em>. 1st ed. Springer International Publishing.</p>
</div>
<div id="ref-Manevitz2001">
<p>Manevitz, L. M., and M. Yousef. 2001. “One-class SVMs for Document Classification.” <em>Journal of Machine Learning Research</em> 2:139–54.</p>
</div>
<div id="ref-Bailey2012">
<p>Bailey, Stephen. 2012. “Principal Component Analysis with Noisy and/or Missing Data.” <em>Publications of the Astronomical Society of the Pacific</em> 124 (September):1015–23.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="methodology.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nonlinear-extensions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["outlier-detection.pdf", "outlier-detection.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
