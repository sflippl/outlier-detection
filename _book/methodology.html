<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>outlier-detection.utf8.md</title>
  <meta name="description" content="This article is concerned with outlier detection using regression in R.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="outlier-detection.utf8.md" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This article is concerned with outlier detection using regression in R." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="outlier-detection.utf8.md" />
  
  <meta name="twitter:description" content="This article is concerned with outlier detection using regression in R." />
  

<meta name="author" content="Samuel Lippl">


<meta name="date" content="2018-09-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="linear.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Outlier Detection using Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction: neurons, models and outliers</a></li>
<li class="chapter" data-level="2" data-path="methodology.html"><a href="methodology.html"><i class="fa fa-check"></i><b>2</b> Methodology</a><ul>
<li class="chapter" data-level="2.1" data-path="methodology.html"><a href="methodology.html#a-nested-account-of-outlier-detection"><i class="fa fa-check"></i><b>2.1</b> A nested account of outlier detection</a></li>
<li class="chapter" data-level="2.2" data-path="methodology.html"><a href="methodology.html#context-probabilistic-and-cluster-analysis-for-outlier-detection"><i class="fa fa-check"></i><b>2.2</b> Context: Probabilistic and cluster analysis for outlier detection</a></li>
<li class="chapter" data-level="2.3" data-path="methodology.html"><a href="methodology.html#z-scoring-a-heuristic-for-normalization-of-outlier-scores"><i class="fa fa-check"></i><b>2.3</b> Z-Scoring: A heuristic for normalization of outlier scores</a></li>
<li class="chapter" data-level="2.4" data-path="methodology.html"><a href="methodology.html#evaluation-of-outlier-detection-algorithms"><i class="fa fa-check"></i><b>2.4</b> Evaluation of outlier detection algorithms</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>3</b> Linear Outlier Detection</a><ul>
<li class="chapter" data-level="3.1" data-path="linear.html"><a href="linear.html#linear-model"><i class="fa fa-check"></i><b>3.1</b> Linear Models</a><ul>
<li class="chapter" data-level="3.1.1" data-path="linear.html"><a href="linear.html#motivation"><i class="fa fa-check"></i><b>3.1.1</b> Motivation</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear.html"><a href="linear.html#definition-and-implementation-in-r"><i class="fa fa-check"></i><b>3.1.2</b> Definition and implementation in R</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear.html"><a href="linear.html#example-application"><i class="fa fa-check"></i><b>3.1.3</b> Example application</a></li>
<li class="chapter" data-level="3.1.4" data-path="linear.html"><a href="linear.html#data-compression-and-correction"><i class="fa fa-check"></i><b>3.1.4</b> Data compression and correction</a></li>
<li class="chapter" data-level="3.1.5" data-path="linear.html"><a href="linear.html#discussion"><i class="fa fa-check"></i><b>3.1.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear.html"><a href="linear.html#linear-pca"><i class="fa fa-check"></i><b>3.2</b> Principal Component Analysis</a><ul>
<li class="chapter" data-level="3.2.1" data-path="linear.html"><a href="linear.html#motivation-1"><i class="fa fa-check"></i><b>3.2.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear.html"><a href="linear.html#definition"><i class="fa fa-check"></i><b>3.2.2</b> Definition</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear.html"><a href="linear.html#example-application-1"><i class="fa fa-check"></i><b>3.2.3</b> Example application</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear.html"><a href="linear.html#data-compression-and-correction-1"><i class="fa fa-check"></i><b>3.2.4</b> Data compression and correction</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear.html"><a href="linear.html#discussion-1"><i class="fa fa-check"></i><b>3.2.5</b> Discussion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html"><i class="fa fa-check"></i><b>4</b> Nonlinear extensions</a><ul>
<li class="chapter" data-level="4.1" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#kernel-pca"><i class="fa fa-check"></i><b>4.1</b> Kernel PCA</a><ul>
<li class="chapter" data-level="4.1.1" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#motivation-2"><i class="fa fa-check"></i><b>4.1.1</b> Motivation</a></li>
<li class="chapter" data-level="4.1.2" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#definition-1"><i class="fa fa-check"></i><b>4.1.2</b> Definition</a></li>
<li class="chapter" data-level="4.1.3" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#example-application-2"><i class="fa fa-check"></i><b>4.1.3</b> Example application</a></li>
<li class="chapter" data-level="4.1.4" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#discussion-2"><i class="fa fa-check"></i><b>4.1.4</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="nonlinear-extensions.html"><a href="nonlinear-extensions.html#neural-networks"><i class="fa fa-check"></i><b>4.2</b> Neural networks</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>5</b> Summary</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methodology" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Methodology</h1>
<p>This chapter introduces the methods with which the outlier detection algorithms are discussed in the following. It first discusses the connection between data compression, data correction, outlier labels and outlier scores, embeds methods using regression in alternatives using probabilistic modeling or clustering and then presents <em>Z-Scoring</em> as a simple heuristic to normalize outlier scores. Finally, evaluation of outlier scores, both in this report and in general are discussed and toy outlier datasets are introduced.</p>
<div id="a-nested-account-of-outlier-detection" class="section level2">
<h2><span class="header-section-number">2.1</span> A nested account of outlier detection</h2>
<p>Outlier detection is closely related to the fields of data compression and data correction. In fact, there is a canonical connection which will be discussed in this section. Firstly, let us produce a definition of the different general algorithms.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-1" class="definition"><strong>Definition 2.1  </strong></span>Consider some data <span class="math inline">\(D\in\mathcal{D}^n,n\in\mathbb{N}\)</span>. An algorithm <span class="math inline">\(A\)</span> achieves</p>
<ol style="list-style-type: lower-alpha">
<li><p><strong>data compression</strong> if and only if it yields an <em>encoding procedure</em>
<span class="math display">\[E^D:\mathcal{D}\to\mathcal{C}\]</span>
(where <span class="math inline">\(\mathcal{C}\)</span> is some compressed space) and a <em>decoding procedure</em>
<span class="math display">\[F^D:\mathcal{C}\to\mathcal{D}.\]</span>
Consider, for instance, a large file <span class="math inline">\(D\in\mathcal{D}\)</span> where <span class="math inline">\(\mathcal{D}\)</span> is the space of files of a certain maximum size. The zipping algorithm DEFLATE uses – broadly speaking – repeating <strong>pattern</strong>s in the files to reduce size by relating repeated <strong>pattern</strong>s to the original <strong>pattern</strong>. For instance, the highlighted string sequence “pattern” in the last sentence occurred thrice – the latter two times, it could have been replaced by a reference to the original sequence. DEFLATE would thus produce a compressed (zipped) file <span class="math inline">\(Z=E^D(D)\)</span> which can then be decoded to yield the original data file <span class="math inline">\(D = F^D(Z)\)</span>. <span class="citation">(see Deutsch <a href="#ref-Group1996">1996</a>)</span></p>
This is an example of <em>lossless</em> data compression. In many cases, however, data is compressed in such a way that it cannot be entirely recovered. This is called <em>lossy</em> data compression. I will illustrate the procedure by a simple example. Suppose that the data consists of a set of geometrical figures. We detect that they are all rectangles and we can therefore save them by the width and height. This data compression is lossless. If we wish to compress the data even further, we might note that most rectangles are approximately squares and we can encode them by the mean of their width and height. If, for instance a rectangle of width 5.0 cm and height 6.0 cm is encoded, we save the number 5.5 (encoded). In the decoding step, we convert the number 5.5 into a square of width and height 5.5 cm. Whereas this is not exactly the rectangle we put in, we may decide that it is close enough.</li>
<li><strong>data correction</strong> if and only if it yields a <em>correction procedure</em>
<span class="math display">\[C^D:\mathcal{D}^n\to\mathcal{D}^n\]</span>
such that <span class="math inline">\(C^D(D)\)</span> is a corrected version of <span class="math inline">\(D\)</span>. In our working example, data correction would imply a different perspective on our rectangle-square-problem. Perhaps, we know that all geometrical forms are supposed to be squares but there has been some measurement error. The product could then be viewed as a computation of the most likely square that has been measured in the first place. Generally, we do not need a framework with encoding and decoding – another solution would be, for example, to assume the lesser value of width and height as the actual value of both width and height.</li>
<li><strong>outlier scoring</strong> if and only if it can label each data point <span class="math inline">\(d\in\mathcal{D}\)</span> with a score that captures the dissimilarity of <span class="math inline">\(d\)</span> compared to the dataset <span class="math inline">\(D\)</span>. This would, again, imply a different perspective: perhaps, not all measured forms have been squares but they have usually been approximate squares. Then, the rectangles with large differences between width and height are unusual observations and some measure like the absolute difference between width and height captures the degree to which they are outliers – their <em>outlier score</em>. Of course, we could also try to measure how different the corrected form (the square) is from the original form (the rectangle). The larger the difference, the larger the outlier score.</li>
<li><strong>outlier labels</strong> if and only if it can label each data point <span class="math inline">\(d\in\mathcal{D}\)</span> with a binary label that indicates whether it is an outlier score or not. For instance, we might define, that those rectangles are outliers where width and height have a difference of more than 1 cm.</li>
</ol>
</div>

<p>Outlier scoring and labeling are a part of outlier detection and we will discuss their relation in the next section. At this point, I will work out the implied relations between the different algorithms, namely:</p>
<ul>
<li>A <em>data compression</em> algorithm can be used for <em>data correction</em> by viewing the encoded and then decoded data as corrected data.</li>
<li>A <em>data correction</em> algorithm can be used for <em>outlier scoring</em> by measuring the dissimilarity between the original and the corrected data.</li>
<li>An <em>outlier scoring</em> algorithm can be used for <em>outlier labels</em> by defining a threshold such that any data point with a larger outlier score is defined as an outlier.</li>
</ul>
<p>This implies a nested account of outlier detection and allows us to further our understanding of capable outlier detection algorithms by regarding them as compressing and correcting algorithms. More explicitly, an unusual observation is one that cannot be compressed as much without a large margin of error, resp. one that differs a lot from its corrected version. Both perspectives therefore contribute to our understanding of outlier detection algorithms and I will regard them where appropriate.</p>
<p>It remains to remark that, of course, data compression algorithms are not, per se, superior to pure outlier detection algorithms just because they also induce a method of the latter category. If the latter algorithm has a better performance in detecting outliers in a certain situation, it is evidently better at this task, even if it is not capable of data compression.</p>
</div>
<div id="context-probabilistic-and-cluster-analysis-for-outlier-detection" class="section level2">
<h2><span class="header-section-number">2.2</span> Context: Probabilistic and cluster analysis for outlier detection</h2>
<p>While this report introduces regression methods for outlier detection, this is not the only option. In its most basic form, outlier detection consists of a collection of real numbers <span class="math inline">\(x_1,\dotsc, x_n\)</span> and the suspicious data points are “too” far at the borders of the dataset. This is the domain of <em>extreme value analysis</em>. A useful outlier score in this context is the standardized deviation from the mean (often referred to as the <em>Z-Score</em>):
<span class="math display">\[z_i=|x_i-\hat{\mu}|/\hat{\sigma}, \hat{\mu}=\frac{\sum_{j=1}^nx_j}{n}, \hat{\sigma}=\sqrt{\frac{\sum_{j=1}^n(x_j-\hat{\mu})^2}{n}}\]</span>
Theoretically, this is motivated by a normal distribution as, in this case, a higher value of <span class="math inline">\(z_i\)</span> implies a lower probability of <span class="math inline">\(x_i\)</span>. However, as long as the anomalies lie at the borders of the data, the score provides a good heuristic even under non-normal assumptions. As a general rule of thumb, Aggarwal suggests that data points with Z scores <span class="math inline">\(z_i&gt;=3\)</span> are to be treated as outliers. <span class="citation">(Aggarwal <a href="#ref-Aggarwal2017">2017</a>, 6)</span></p>
<p>Probabilistic outlier detection can also handle more complex data, the elementary assumption being that unusual data is improbable data <span class="citation">(for a general introduction, see Aggarwal <a href="#ref-Aggarwal2017">2017</a>, ch. 2)</span>. An important application is the multivariate analog of the univariate Z-score: if <span class="math inline">\(\mu\in\mathbb{R}^d\)</span> is the (estimated or actual) mean and <span class="math inline">\(\Sigma\in\mathbb{R}^{d\times d}\)</span> is the (estimated or actual) covariance matrix, we define the <em>Mahalanobis score</em> for an observation <span class="math inline">\(x\in\mathbb{R}^d\)</span> by</p>
<p><span class="math display">\[M(x,\mu,\Sigma):=\sqrt{(x-\mu)^T\Sigma^{-1}(x-\mu)}\]</span></p>
<p>If the data follows a multivariate normal distribution, a higher score, again, implies a lower probability. The Mahalanobis score also accounts for correlations within the data – other methods like the <em>Expectation Maximization algorithm</em> are able to capture arbitrarily complex patterns in the data <span class="citation">(Dempster, Laird, and Rubin <a href="#ref-Dempster1977">1977</a>)</span>.</p>
<p>What use do regression methods have, then? I would argue that there is a two-fold shift. Firstly, regression methods do not always model probabilities. As seen in the next chapter, there are certain circumstances in which its probabilities are not the most important determinant of whether a data point is an outlier. Secondly, the focus on the relation between the different dimensions of the data yields a higher explanatory power of the outlier detection score. In contrast, probabilistic methods are rendered inflexible by certain assumptions, namely a data point is an outlier if it has low probability.</p>
<p>Analogously, <em>clustering outlier detection</em> focus on the connection between observations instead of dimensions. They determine usual regions within the dataset, i. e. <em>clusters</em>. A data point is an outlier if it does not belong to any cluster. A general introduction can be found in Aggarwal <span class="citation">(<a href="#ref-Aggarwal2017">2017</a>, ch. 4)</span>.</p>
<p>Finally, there is an important application of extreme value analysis within regression outlier detection. Many of these methods yield outlier scores. However, these scores are scaled differently and it is therefore not immediately clear what values correspond to outliers. However, only data points with particularly high scores can be outliers – the scores therefore adhere to the assumptions of extreme value analysis. The next section will present a general heuristic that uses Z-scoring to normalize outlier scores.</p>
</div>
<div id="z-scoring-a-heuristic-for-normalization-of-outlier-scores" class="section level2">
<h2><span class="header-section-number">2.3</span> Z-Scoring: A heuristic for normalization of outlier scores</h2>
<p>Oftentimes, outlier detection algorithms yields scores which capture some kind of absolute deviation from the expected value – the value is nonnegative and a larger value implies that the data point conforms less the patterns in the data. However, the orders of magnitude may be very different: a value of 1 might imply a strict outlier in one case whereas a value of 100 might be well within the expected deviations in another case. I present a heuristic that presupposes that any outlier score <span class="math inline">\(o_i\)</span> of a data point <span class="math inline">\(i\)</span> can be written as</p>
<p><span class="math display">\[
o_i=|\tilde{x}_i-\tilde{\mu}|
\]</span></p>
<p>where <span class="math inline">\(\tilde{\mu}\)</span> is the expected value of <span class="math inline">\(\tilde{x}_i\)</span>. Clearly, this corresponds to the intuition of outlier scores that the larger value implies a stronger possibility that <span class="math inline">\(i\)</span> is indeed an outlier. We can now standardize this through the means of <em>extreme value analysis</em>: the latent variable <span class="math inline">\(\tilde{\mu}\)</span> does not even have to be estimated. On the other hand, <span class="math inline">\(\sigma\)</span>, the standard deviation of <span class="math inline">\(\tilde{x}_i\)</span> may be estimated by</p>
<p><span class="math display">\[
\hat{\sigma}=
\frac{1}{n}\sum_{i=1}^n(\tilde{x}_i-\tilde{\mu})^2=
\frac{1}{n}\sum_{i=1}^n|\tilde{x}_i-\tilde{\mu}|^2=
\frac{1}{n}\sum_{i=1}^no_i^2,
\]</span></p>
<p>which implies that we do not need the latent variable <span class="math inline">\(\tilde{\mu}\)</span> for the estimation. Note that we have normalized by <span class="math inline">\(n\)</span> and not <span class="math inline">\(n-1\)</span> as we do not lose a degree of freedom because we do not estimate the mean <span class="math inline">\(\tilde{\mu}\)</span>.</p>
<p>Under our presuppositions, we have therefore now determined a value which broadly conforms to standardized rules of thumb: more specifically we will look at the thresholds <span class="math inline">\(2\)</span> (narrow), <span class="math inline">\(3\)</span> (recommended by Aggarwal) and <span class="math inline">\(4\)</span> (rather high). The aforementioned <em>Z-plot</em> visualizes this standardization in a consistent way by using the following palette from the package <code>scico</code> <span class="citation">(Pedersen and Crameri <a href="#ref-scico">2018</a>)</span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">out_pal &lt;-<span class="st"> &quot;lajolla&quot;</span></a></code></pre></div>
<p>Furthermore, we will cap the values at <span class="math inline">\(4\)</span> by assuming that every value above this threshold can safely be considered an outlier. This is ensured by the following transformation:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">out_breaks &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">out_trans &lt;-<span class="st"> </span><span class="kw">trans_new</span>(<span class="st">&quot;out&quot;</span>, <span class="dt">transform =</span> <span class="cf">function</span>(x) <span class="kw">pmax</span>(<span class="kw">pmin</span>(x, <span class="dv">4</span>), <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb2-3" data-line-number="3">                       <span class="dt">inverse =</span> <span class="cf">function</span>(x) x,</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">                       <span class="dt">breaks =</span> <span class="cf">function</span>(limits) <span class="dv">0</span><span class="op">:</span><span class="dv">4</span>,</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">                       <span class="dt">minor_breaks =</span> <span class="cf">function</span>(limits) <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="fl">.5</span>))</a></code></pre></div>
<p>We therefore define the following visualization as a Z-plot:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">err_marg &lt;-<span class="st"> </span><span class="fl">.05</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">zplot &lt;-<span class="st"> </span><span class="cf">function</span>(df, x, y, outlier, <span class="dt">errmarg =</span> err_marg, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">  <span class="kw">ggplot</span>(df, <span class="kw">aes_string</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">fill =</span> outlier)) <span class="op">+</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_tile</span>(<span class="dt">show.legend =</span> show.legend) <span class="op">+</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5"><span class="st">    </span><span class="kw">scale_fill_scico</span>(<span class="dt">palette =</span> out_pal, <span class="dt">trans =</span> out_trans, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">4</span>),</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">                     <span class="dt">labels =</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">                       <span class="cf">function</span>(breaks) <span class="kw">paste</span>(breaks, <span class="st">&quot;s&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb3-8" data-line-number="8"><span class="st">    </span>my_theme <span class="op">+</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9"><span class="st">    </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;top&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb3-10" data-line-number="10"><span class="st">    </span><span class="kw">geom_tile</span>(<span class="dt">data =</span> df[<span class="kw">abs</span>(df[[outlier]] <span class="op">-</span><span class="st"> </span><span class="dv">3</span>) <span class="op">&lt;=</span><span class="st"> </span>errmarg, ],</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">              <span class="dt">fill =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb3-12" data-line-number="12"><span class="st">    </span><span class="kw">geom_tile</span>(<span class="dt">data =</span> df[<span class="kw">abs</span>(df[[outlier]] <span class="op">-</span><span class="st"> </span><span class="dv">2</span>) <span class="op">&lt;=</span><span class="st"> </span>errmarg <span class="op">|</span></a>
<a class="sourceLine" id="cb3-13" data-line-number="13"><span class="st">                                     </span><span class="kw">abs</span>(df[[outlier]] <span class="op">-</span><span class="st"> </span><span class="dv">4</span>) <span class="op">&lt;=</span><span class="st"> </span>errmarg, ],</a>
<a class="sourceLine" id="cb3-14" data-line-number="14">              <span class="dt">fill =</span> <span class="st">&quot;gray&quot;</span>)</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">}</a></code></pre></div>
<p>Let us look at the simple example in figure <a href="methodology.html#fig:norm-ex">2.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:norm-ex"></span>
<img src="outlier-detection_files/figure-html/norm-ex-1.png" alt="Example for a Z-plot: the \(L_2\)-norm" width="50%" />
<p class="caption">
Figure 2.1: Example for a Z-plot: the <span class="math inline">\(L_2\)</span>-norm
</p>
</div>

<p>Any value above <span class="math inline">\(4\)</span> is now simply black which allows the plot to be compared across outlier detection algorithms as the colourbar remains constant. Furthermore, the inner gray circle refers to a threshold of <span class="math inline">\(2\)</span> whereas the outer gray circle refers to a threshold of <span class="math inline">\(4\)</span>. The black circle in the middle corresponds to the value <span class="math inline">\(3\)</span>. These are intended to visualize the outlier areas according to these three thresholds more clearly. Together, the Z-plot provides a good first look into the results of a certain outlier detection algorithm in two dimensions. We will therefore now discuss two-dimensional example data.</p>
</div>
<div id="evaluation-of-outlier-detection-algorithms" class="section level2">
<h2><span class="header-section-number">2.4</span> Evaluation of outlier detection algorithms</h2>
<p>The discussion of anomaly benchmark datasets by <span class="citation">Emmott et al. (<a href="#ref-Emmott2015">2015</a>)</span> has a few implications for the evaluation of algorithms in this report: firstly, they point out that artificial data is almost always too simple and therefore not fit for a fair evaluation of outlier detection algorithms. They therefore advocate constructing benchmark data for this evaluation by modifying real datasets. In terms of characterizing these datasets, they identify four problem dimensions of anomaly detection:</p>
<ul>
<li><em>point difficulty</em>: how different are the outliers from the normal data points?</li>
<li><em>semantic variation</em>: how different are the processes which create the different outliers? If many outliers are generated by the same process, this patterns might be captured by the algorithm which would yield unfavorable results.</li>
<li><em>relative frequency</em>: how many data points are anomalies of interest?</li>
<li><em>feature relevance</em>: how many of the feature dimensions are relevant/irrelevant for the task at hand?</li>
</ul>
<p>The scope of this report is too narrow to include a detailed evaluation of the different algorithms. Instead, I will refer to other evaluations talk about influences of these four dimensions where they are evident. As a small visualization of the algorithms, I will introduce four datasets with different difficulties and outlier patterns. These can be seen in figure <a href="methodology.html#fig:ex-data">2.2</a>.</p>
<div class="figure"><span id="fig:ex-data"></span>
<img src="outlier-detection_files/figure-html/ex-data-1.png" alt="Example datasets" width="100%" />
<p class="caption">
Figure 2.2: Example datasets
</p>
</div>
<p>In the following chapters, I will distinguish outliers and normal data points by shape.</p>
<p>Clearly, these datasets represent different levels of difficulty – regarding the sinusoidal pattern, there might not even be accordance on the outliers between humans. Furthermore, I do not claim that these datasets represent a fair evaluation of the algorithms. They are only intended as simple examples of application. I will therefore not report the detection accuracy as this would implicitly rank the algorithms without a proper methodology. I also point out that the linear, circular and quadratic patterns are very difficult in terms of their little semantic variation.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Group1996">
<p>Deutsch, P. 1996. “DEFLATE Compressed Data Format Specification version 1.3.” <a href="https://doi.org/10.17487/rfc1951" class="uri">https://doi.org/10.17487/rfc1951</a>.</p>
</div>
<div id="ref-Aggarwal2017">
<p>Aggarwal, Charu C. 2017. <em>Outlier Analysis</em>. Springer Publishing Company, Incorporated.</p>
</div>
<div id="ref-Dempster1977">
<p>Dempster, A P, N M Laird, and D B Rubin. 1977. “Maximum Likelihood from Incomplete Data via the EM Algorithm.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 39 (1). [Royal Statistical Society, Wiley]:1–38. <a href="http://www.jstor.org/stable/2984875" class="uri">http://www.jstor.org/stable/2984875</a>.</p>
</div>
<div id="ref-scico">
<p>Pedersen, Thomas Lin, and Fabio Crameri. 2018. <em>scico: Colour Palettes Based on the Scientific Colour-Maps</em>. <a href="https://cran.r-project.org/package=scico" class="uri">https://cran.r-project.org/package=scico</a>.</p>
</div>
<div id="ref-Emmott2015">
<p>Emmott, Andrew, Shubhomoy Das, Thomas G Dietterich, Alan Fern, and Weng-Keen Wong. 2015. “Systematic Construction of Anomaly Detection Benchmarks from Real Data.” <em>CoRR</em> abs/1503.0. <a href="http://arxiv.org/abs/1503.01158" class="uri">http://arxiv.org/abs/1503.01158</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["outlier-detection.pdf", "outlier-detection.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
