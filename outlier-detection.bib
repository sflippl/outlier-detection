@manual{R-psych,
address = {Evanston, Illinois},
annote = {R package version 1.8.4},
author = {Revelle, William},
organization = {Northwestern University},
title = {{psych: Procedures for Psychological, Psychometric, and Personality Research}},
url = {https://cran.r-project.org/package=psych},
year = {2018}
}
@book{Galton1889,
author = {Galton, Francis},
publisher = {Macmillan},
series = {Natural Inheritance},
title = {{Natural Inheritance}},
url = {https://books.google.de/books?id=pGIVAAAAYAAJ},
year = {1889}
}
@incollection{sep-mathematics-constructive,
author = {Bridges, Douglas and Palmgren, Erik},
booktitle = {The Stanford Encyclopedia of Philosophy},
edition = {Summer 201},
editor = {Zalta, Edward N},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/The Stanford Encyclopedia of Philosophy/Bridges, Palmgren_2018_Constructive Mathematics.pdf:pdf},
howpublished = {\url{https://plato.stanford.edu/archives/sum2018/entries/mathematics-constructive/}},
publisher = {Metaphysics Research Lab, Stanford University},
title = {{Constructive Mathematics}},
year = {2018}
}
@book{ggplot2,
author = {Wickham, Hadley},
isbn = {978-3-319-24277-4},
publisher = {Springer-Verlag New York},
title = {{ggplot2: Elegant Graphics for Data Analysis}},
url = {http://ggplot2.org},
year = {2016}
}
@misc{ODDS,
author = {Rayana, Shebuti},
publisher = {Stony Brook University, Department of Computer Sciences},
title = {{ODDS Library}},
url = {http://odds.cs.stonybrook.edu},
year = {2016}
}
@manual{Rmatlab,
annote = {R package version 3.6.1},
author = {Bengtsson, Henrik},
title = {{R.matlab: Read and Write MAT Files and Call MATLAB from Within R}},
url = {https://cran.r-project.org/package=R.matlab},
year = {2016}
}
@manual{scico,
annote = {R package version 1.0.0},
author = {Pedersen, Thomas Lin and Crameri, Fabio},
title = {{scico: Colour Palettes Based on the Scientific Colour-Maps}},
url = {https://cran.r-project.org/package=scico},
year = {2018}
}
@article{Wickham2014,
abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
annote = {Wickham introduces the concept of tidy data which underlies the tidyverse in R. It is an attempt to formulate his intuition regarding a subset of data cleaning he calls data tidying. He describes these operations with the plyr-package that is, as of today, outdated. The general thought process still applies: Observations are rows, variables are column, different kinds of observations are different tables. The ideas are inspired by Codd's relational algebra (1990) which I will look into next.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
eprint = {arXiv:1501.0228},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Journal of Statistical Software/Wickham_2014_Tidy Data.pdf:pdf},
isbn = {9780387781662},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {10},
pages = {1--23},
pmid = {18291371},
title = {{Tidy Data}},
url = {http://www.jstatsoft.org/v59/i10/},
volume = {59},
year = {2014}
}
@article{Bornmann2011,
archivePrefix = {arXiv},
arxivId = {1104.0807},
author = {Bornmann, Lutz and Marx, Werner},
eprint = {1104.0807},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/CoRR/Bornmann, Marx_2011_The Anna Karenina principle {A} mechanism for the explanation of success in science.pdf:pdf},
journal = {CoRR},
title = {{The Anna Karenina principle: {A} mechanism for the explanation of success in science}},
url = {http://arxiv.org/abs/1104.0807},
volume = {abs/1104.0},
year = {2011}
}
@book{Diamond1997,
address = {New York, NY, USA},
author = {Diamond, Jared},
publisher = {W. W. Norton},
title = {{Guns, Germs and Steel: The Fate of Human Society}},
year = {1997}
}
@inproceedings{Scholkopf1999,
address = {Cambridge, MA, USA},
author = {Sch{\"{o}}lkopf, Bernhard and Williamson, Robert and Smola, Alex and Shawe-Taylor, John and Platt, John},
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Proceedings of the 12th International Conference on Neural Information Processing Systems/Sch{\"{o}}lkopf et al._1999_Support Vector Method for Novelty Detection.pdf:pdf},
pages = {582--588},
publisher = {MIT Press},
series = {NIPS'99},
title = {{Support Vector Method for Novelty Detection}},
url = {http://dl.acm.org/citation.cfm?id=3009657.3009740},
year = {1999}
}
@article{Emmott2015,
archivePrefix = {arXiv},
arxivId = {1503.01158},
author = {Emmott, Andrew and Das, Shubhomoy and Dietterich, Thomas G and Fern, Alan and Wong, Weng-Keen},
eprint = {1503.01158},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/CoRR/Emmott et al._2015_Systematic Construction of Anomaly Detection Benchmarks from Real Data.pdf:pdf},
journal = {CoRR},
title = {{Systematic Construction of Anomaly Detection Benchmarks from Real Data}},
url = {http://arxiv.org/abs/1503.01158},
volume = {abs/1503.0},
year = {2015}
}
@article{Tax2004,
abstract = {Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.},
author = {Tax, David M J and Duin, Robert P W},
doi = {10.1023/B:MACH.0000008084.60811.49},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Machine Learning/Tax, Duin_2004_Support Vector Data Description.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
month = {jan},
number = {1},
pages = {45--66},
title = {{Support Vector Data Description}},
url = {https://doi.org/10.1023/B:MACH.0000008084.60811.49},
volume = {54},
year = {2004}
}
@misc{Domke2010,
author = {Domke, Justin},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Unknown/Domke_2010_Lagrange Duality.pdf:pdf},
pages = {1--4},
title = {{Lagrange Duality}},
url = {https://people.cs.umass.edu/$\sim$domke/courses/sml2010/06lagrange.pdf},
year = {2010}
}
@book{Tolstoy2009,
author = {Tolstoy, Lew},
editor = {Tietze, Rosemarie},
publisher = {Carl Hanser Verlag},
title = {{Anna Karenina}},
year = {2009}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Science/Hinton, Salakhutdinov_2006_Reducing the Dimensionality of Data with Neural Networks.pdf:pdf},
issn = {0036-8075},
journal = {Science},
number = {5786},
pages = {504--507},
publisher = {American Association for the Advancement of Science},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
url = {http://science.sciencemag.org/content/313/5786/504},
volume = {313},
year = {2006}
}
@unpublished{vdem2018,
author = {Coppedge, Michael and Gerring, John and {Knutsen, Carl Henrik Lindberg}, Staffan I. and Skaaning, Svend-Erik and Teorell, Jan and Altman, David and Bernhard, Michael and Steven, M. and Cornell, Agnes and Dahlum, Sirianne and Gjerl{\o}w, Haakon and Glynn, Adam and Hicken, Allen and Krusell, Joshua and L{\"{u}}hrmann, Anna and Marquardt, Kyle L. and McMann, Kelly and Mechkova, Valeriya and Medzihorsky, Juraj and Olin, Moa and Paxton, Pamela and Pemstein, Daniel and Pernes, Josefine and von R{\"{o}}mer, Johannes and Seim, Brigitte and Sigman, Rachel and Staton, Jeffrey and Stepanova, Natalia and Sundstr{\"{o}}m, Aksel and Tzelgov, Eitan and Wang, Yi-ting and Wig, Tore and Wilson, Steven and Ziblatt, Daniel},
institution = {Varieties of Democracy (V-Dem) Project},
title = {{V-Dem [Country-Year/Country-Date] Dataset v8}},
year = {2018}
}
@article{Keller2012,
abstract = {Outlier mining is a major task in data analysis. Outliers are objects that highly deviate from regular objects in their local neighborhood. Density-based outlier ranking methods score each object based on its degree of deviation. In many applications, these ranking methods degenerate to random listings due to low contrast between outliers and regular objects. Outliers do not show up in the scattered full space, they are hidden in multiple high contrast subspace projections of the data. Measuring the contrast of such subspaces for outlier rankings is an open research challenge. In this work, we propose a novel subspace search method that selects high contrast subspaces for density-based outlier ranking. It is designed as pre-processing step to outlier ranking algorithms. It searches for high contrast subspaces with a significant amount of conditional dependence among the subspace dimensions. With our approach, we propose a first measure for the contrast of subspaces. Thus, we enhance the quality of traditional outlier rankings by computing outlier scores in high contrast projections only. The evaluation on real and synthetic data shows that our approach outperforms traditional dimensionality reduction techniques, naive random projections as well as state-of-the-art subspace search techniques and provides enhanced quality for outlier ranking.},
author = {Keller, Fabian and M{\"{u}}ller, Emmanuel and B{\"{o}}hm, Klemens},
doi = {10.1109/ICDE.2012.88},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Proceedings - International Conference on Data Engineering/Keller, M{\"{u}}ller, B{\"{o}}hm_2012_HiCS High contrast subspaces for density-based outlier ranking.pdf:pdf},
isbn = {1063-6382},
issn = {10844627},
journal = {Proceedings - International Conference on Data Engineering},
number = {1},
pages = {1037--1048},
title = {{HiCS: High contrast subspaces for density-based outlier ranking}},
year = {2012}
}
@article{Sundberg2013a,
abstract = {This article presents the UCDP Georeferenced Event Dataset (UCDP GED). The UCDP GED is an event dataset that disaggregates three types of organized violence (state-based conflict, non-state conflict, and one-sided violence) both spatially and temporally. Each event – defined as an instance of organized violence with at least one fatality – comes with date, geographical location, and identifiers that allow the dataset to be linked to and merged with other UCDP datasets. The first version of the dataset covers events of fatal violence on the African continent between 1989 and 2010. This article, firstly, introduces the rationale for the new dataset, and explains the basic coding procedures as well as the quality controls. Secondly, we discuss some of the data's potential weaknesses in representing the universe of organized violence, as well as some potential biases induced by the operationalizations. Thirdly, we provide an example of how the data can be used, by illustrating the association between cities and organized violence, taking population density into account. The UCDP GED is a useful resource for conflict analyses below the state and country-year levels, and can provide us with new insights into the geographical determinants and temporal sequencing of warfare and violence.},
author = {Sundberg, Ralph and Melander, Erik},
doi = {10.1177/0022343313484347},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Journal of Peace Research/Sundberg, Melander_2013_Introducing the UCDP Georeferenced Event Dataset(2).pdf:pdf},
isbn = {0022-3433},
issn = {00223433},
journal = {Journal of Peace Research},
keywords = {Africa,dataset,disaggregation,events data,georeferencing,organized violence},
number = {4},
pages = {523--532},
title = {{Introducing the UCDP Georeferenced Event Dataset}},
volume = {50},
year = {2013}
}
@unpublished{Pemstein2018,
address = {University of Gothenburg},
author = {Pemstein, Daniel and Marquardt, Kyle L. and Tzelgov, Eitan and Wang, Yi-ting and Krusell, Joshua and Miri, Farhad},
institution = {Varieties of Democracy Institute},
title = {{The V-Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data}},
year = {2018}
}
@article{Tenenbaum2000,
author = {Tenenbaum, Joshua B and Silva, Vin De and Langford, John C},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Science/Tenenbaum, Silva, Langford_2000_A Global Geometric Framework for Nonlinear Dimensionality Reduction.pdf:pdf},
journal = {Science},
pages = {2319--2323},
title = {{A Global Geometric Framework for Nonlinear Dimensionality Reduction}},
volume = {290},
year = {2000}
}
@techreport{Croicu2017,
author = {Croicu, Mihai and Sundberg, Ralph},
institution = {Department of Peace and Conflict Research, Uppsala University},
title = {{UCDP GED Codebook version 18.1}},
year = {2017}
}
@inproceedings{Weiss1999,
author = {Weiss, Y},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790354},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Proceedings of the Seventh IEEE International Conference on Computer Vision/Weiss_1999_Segmentation using eigenvectors a unifying view.pdf:pdf},
keywords = {Algorithm design and analysis,Clustering algorithms,Computer vision,Humans,Image analysis,Image segmentation,Layout,Parameter estimation,Performance analysis,Stability,affinity matrix,algorithm performance,automatic image grouping,automatic image segmentation,block matrices,computer vision,eigendecomposition algorithms,eigenvalues and eigenfunctions,eigenvectors,image segmentation,matrix algebra,real images,stability,synthetic images},
pages = {975--982 vol.2},
title = {{Segmentation using eigenvectors: a unifying view}},
volume = {2},
year = {1999}
}
@article{article,
author = {Hoffmann, Heiko},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Pattern Recognition/Hoffmann_2007_Kernel PCA for novelty detection.pdf:pdf},
journal = {Pattern Recognition},
pages = {863--874},
title = {{Kernel PCA for novelty detection}},
volume = {40},
year = {2007}
}
@inproceedings{Tax2002,
abstract = {In one-class classification one tries to describe a class of target data and to distinguish it from all other possible outlier objects. Obvious applications are areas where outliers are very diverse or very difficult or expensive to measure, such as in machine diagnostics or in medical applications. In order to have a good distinction between the target objects and the outliers, good representation of the data is essential. The performance of many one-class classifiers critically depends on the scaling of the data and is often harmed by data distributions in (nonlinear) subspaces. This paper presents a simple preprocessing method which actively tries to map the data to a spherical symmetric cluster and is almost insensitive to data distributed in subspaces. It uses techniques from Kernel PCA to rescale the data in a kernel feature space to unit variance. This transformed data can now be described very well by the Support Vector Data Description, which basically fits a hypersphere around the data. The paper presents the methods and some preliminary experimental results.},
address = {Berlin, Heidelberg},
author = {Tax, David M J and Juszczak, Piotr},
booktitle = {Pattern Recognition with Support Vector Machines},
editor = {Lee, Seong-Whan and Verri, Alessandro},
isbn = {978-3-540-45665-0},
pages = {40--52},
publisher = {Springer Berlin Heidelberg},
title = {{Kernel Whitening for One-Class Classification}},
year = {2002}
}
@book{Aggarwal2017,
author = {Aggarwal, Charu C},
isbn = {1461463955, 9781461463955},
publisher = {Springer Publishing Company, Incorporated},
title = {{Outlier Analysis}},
year = {2017}
}
@article{Scholkopf1998,
author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander and Smola, Er and M{\"{u}}ller, Klaus-Robert},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Neural Computation/Sch{\"{o}}lkopf et al._1998_Nonlinear Component Analysis as a Kernel Eigenvalue Problem.pdf:pdf},
journal = {Neural Computation},
pages = {1299--1319},
title = {{Nonlinear Component Analysis as a Kernel Eigenvalue Problem}},
volume = {10},
year = {1998}
}
@inbook{Sathe2016,
author = {Sathe, Saket and Aggarwal, Charu},
booktitle = {Proceedings of the 2016 SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611974348.20},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Proceedings of the 2016 SIAM International Conference on Data Mining/Sathe, Aggarwal_Unknown_LODES Local Density Meets Spectral Outlier Detection.pdf:pdf},
pages = {171--179},
title = {{LODES: Local Density Meets Spectral Outlier Detection}},
url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974348.20}
}
@article{Zhu2016,
abstract = {This article discusses the material in relation to iForest (Liu et al. in ACM Trans Knowl Discov Data 6(1):3, 2012) reported in a recent Machine Learning Journal paper by Paulheim and Meusel (Mach Learn 100(2--3):509--531, 2015). It presents an empirical comparison result of iForest using the default parameter settings suggested by its creator (Liu et al. 2012) and iForest using the settings employed by Paulheim and Meusel (2015). This comparison has an impact on the conclusion made by Paulheim and Meusel (2015).},
author = {Zhu, Ye and Ting, Kai Ming},
doi = {10.1007/s10994-016-5566-8},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Machine Learning/Zhu, Ting_2016_Commentary a decomposition of the outlier detection problem into a set of supervised learning problems.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
month = {nov},
number = {2},
pages = {301--304},
title = {{Commentary: a decomposition of the outlier detection problem into a set of supervised learning problems}},
url = {https://doi.org/10.1007/s10994-016-5566-8},
volume = {105},
year = {2016}
}
@article{Bailey2012,
author = {Bailey, Stephen},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Publications of the Astronomical Society of the Pacific/Bailey_2012_Principal Component Analysis with Noisy andor Missing Data.pdf:pdf},
journal = {Publications of the Astronomical Society of the Pacific},
number = {September},
pages = {1015--1023},
title = {{Principal Component Analysis with Noisy and/or Missing Data}},
volume = {124},
year = {2012}
}
@inproceedings{Aggarwal2001,
address = {New York, NY, USA},
author = {Aggarwal, Charu C},
booktitle = {Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
doi = {10.1145/375551.383213},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Proceedings of the Twentieth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems/Aggarwal_2001_On the Effects of Dimensionality Reduction on High Dimensional Similarity Search.pdf:pdf},
isbn = {1-58113-361-8},
pages = {256--266},
publisher = {ACM},
series = {PODS '01},
title = {{On the Effects of Dimensionality Reduction on High Dimensional Similarity Search}},
url = {http://doi.acm.org/10.1145/375551.383213},
year = {2001}
}
@inproceedings{Ng2001,
author = {Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
booktitle = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS/Ng, Jordan, Weiss_2001_On Spectral Clustering Analysis and an algorithm.pdf:pdf},
pages = {849--856},
publisher = {MIT Press},
title = {{On Spectral Clustering: Analysis and an algorithm}},
year = {2001}
}
@misc{UCI,
author = {Dheeru, Dua and {Karra Taniskidou}, Efi},
institution = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository}},
year = {2017}
}
@article{Paulheim2015,
abstract = {Outlier detection methods automatically identify instances that deviate from the majority of the data. In this paper, we propose a novel approach for unsupervised outlier detection, which re-formulates the outlier detection problem in numerical data as a set of supervised regression learning problems. For each attribute, we learn a predictive model which predicts the values of that attribute from the values of all other attributes, and compute the deviations between the predictions and the actual values. From those deviations, we derive both a weight for each attribute, and a final outlier score using those weights. The weights help separating the relevant attributes from the irrelevant ones, and thus make the approach well suitable for discovering outliers otherwise masked in high-dimensional data. An empirical evaluation shows that our approach outperforms existing algorithms, and is particularly robust in datasets with many irrelevant attributes. Furthermore, we show that if a symbolic machine learning method is used to solve the individual learning problems, the approach is also capable of generating concise explanations for the detected outliers.},
author = {Paulheim, Heiko and Meusel, Robert},
doi = {10.1007/s10994-015-5507-y},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Machine Learning/Paulheim, Meusel_2015_A decomposition of the outlier detection problem into a set of supervised learning problems.pdf:pdf},
issn = {1573-0565},
journal = {Machine Learning},
month = {sep},
number = {2},
pages = {509--531},
title = {{A decomposition of the outlier detection problem into a set of supervised learning problems}},
url = {https://doi.org/10.1007/s10994-015-5507-y},
volume = {100},
year = {2015}
}
@article{Micenkova2014,
author = {Micenkov{\'{a}}, Barbora and McWilliams, Brian and Assent, Ira},
file = {:C\:/Users/samue/Documents/Studium/Literatur/Papers/Proc. of the ACM SIGKDD Workshop on Outlier Detection and Description, ODD/Micenkov{\'{a}}, McWilliams, Assent_2014_Learning Outlier Ensembles The Best of Both Worlds -- Supervised and Unsupervised.pdf:pdf},
isbn = {9781450329989},
journal = {Proc. of the ACM SIGKDD Workshop on Outlier Detection and Description, ODD.},
keywords = {detection,feature construction,outlier detection,outlier ensembles,semi-supervised outlier},
pages = {1--4},
title = {{Learning Outlier Ensembles : The Best of Both Worlds -- Supervised and Unsupervised}},
year = {2014}
}
@manual{R,
address = {Vienna, Austria},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org}
}
